---
title: "Forest"
author: "Stephen Oates"
date: "Wednesday, September 24, 2014"
output: html_document
---

This post outlines an approach to a competition on Kaggle. Kaggle for those who don't know is a site that hosts data analytics  competitions. It was started by an Australian Economist in 2010, though now it is located in San Francisco. The competitions are often sponsored by companies, and you can win prizes or even job interviews. One of the amazing things about the site is that the competitions are so varied, everything from marking essays, search for dark matter in astronomical data, classifying sounds as whale calls to travel times on a high way. This shows that often data science approaches are agnostic to the field that is being studied, which is part of the fun of the subject I think. The Forest Cover competition is a "Knowledge" competition, so its not a formal competition in that there is a prize. Its still a fun way to practice machine learning.

The competition I am looking at Today is "Forest Cover Type Prediction". The goal of our model is to predict the type of Forest Cover for a particular 30m x 30m patch of ground in one of four wilderness areas in the Roosevelt National Forest in Colorado. There are seven types of Forest coverage:

Forest Coverage Categories

1. Spruce/Fir
2. Lodgepole Pine
3. Ponderosa Pine
4. Cottonwood/Willow
5. Aspen
6. Douglas-fir
7. Krummholz

The variables we are given are:

1. Elevation 
2. Aspect 
3. Slope 
4. Horizontal Distance To Hydrology (Dist to nearest surface water features)
5. Vertical Distance To Hydrology 
6. Horizontal Distance To Roadways 
7. Hillshade 9am 
8. Hillshade Noon 
9. Hillshade 3pm 
10. Horizontal Distance To Fire Points (nearest wildfire ignition points)
11. Wilderness Area Designation
12. Soil Type 

The dataset is very small (~2MB for the training set and ~74MB for the test set).

The task is a straight forward "Supervised Learning" task. We tune our models on the training set where the Forest Cover is known and then use these models to guess the Forest Cover in the training set.


```{r, cache=TRUE, echo=FALSE}
train <- read.csv("E:/Github Stuff/srepho.github.io/Forest/train.csv", header = T)
test <- read.csv("E:/Github Stuff/srepho.github.io/Forest/test.csv", header = T)
sampleSubmission <- read.csv("E:/Github Stuff/srepho.github.io/Forest/sampleSubmission.csv", header = T)
SN <- read.csv("E:/Github Stuff/srepho.github.io/Forest/SN.txt", header=T, sep="\t")
```

After loading the data the next thing is to load up the libraries. The folowing won't all be used, but they are my standard list of useful R packages for Kaggling.


```{r, cache=TRUE}
library(plyr)
library(dplyr)
library(ggplot2)
library(tidyr)
library(caret)
library(forecast)
library(stringr)
```

Now we need to have a look at the data. We are looking in particular at:

1. Do we have missing values? Are these missing values random?
2. Are the values correct?
3. Is the shape of the data tidy?
4. Do we have numerical fields that are factors?
5. What is the shape of the distributions of variables? Will we need to transform them?
6. Are the variables in the train and test set simialir?
7. Are there text values that need to be dealt with?


```{r, cache=TRUE}
summary(train)
summary(test)
```

We can see immediatly that a the fields related to soil and wilderness are binarized versions of categorical type variables. In R we don't need to keep the data in this format (though in Python we would) so to make it easier we can just convert it back to just a single variable. Firstly though lets double check that they can have only one category.

```{r, cache=TRUE}
foo <- select(train, Soil_Type1:Soil_Type40)
foo$rowsum <- rowSums(foo)
summary(foo$rowsum)
```

Great, so we can see that they only have 1 soil type for each row so we can collapse it into one column. I created a quick data frame with the soil names and loaded it as SN. (Have a think about why we want the text of the description).


```{r, cache=TRUE}
i<-1
while(i<41){
foo[,i]<-foo[,i]*i
i<-i+1
}
foo$rowsum <- rowSums(foo)
i<-1
while(i<41){
  foo$rowsum<-str_replace_all(as.character(foo$rowsum), as.character(i), SN$Soil[i])
  i<-i+1
}
train$SoilType<-foo$rowsum
train<-select(train, -(Soil_Type1:Soil_Type40))
```

Now lets do the same thing for the Wilderness Areas but we won't worry about actually copying over the text in this case.

```{r, cache=TRUE}
foo<-select(train, Wilderness_Area1:Wilderness_Area4)

voo<-rowSums(foo)
summary(voo)

i<-1
while(i<5){
foo[,i]<-foo[,i]*i
i<-i+1
}
foo$rowsum <- rowSums(foo)

train$WildernessType<-foo$rowsum
train$WildernessType<-as.factor(train$WildernessType)
train<-select(train, -(Wilderness_Area1:Wilderness_Area4))
```

Just to be neat I will reorder the columns again and change cover type to a factor.

```{r, cache=TRUE}
train<-select(train, Id:Horizontal_Distance_To_Fire_Points, SoilType:WildernessType, Cover_Type)
train$Cover_Type<-as.factor(train$Cover_Type)
summary(train)
```



