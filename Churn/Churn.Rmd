Churn Analysis
========================================================


### What We Are Trying To Do
One of the more common tasks in Business Analytics is to try and understand consumer behaviour. By understanding the hope is that a company can better change this behaviour. In many industries it is more expensive to find a new customer then to entice an existing one to stay. This is usually known as "churn" analysis. The aim is to accurately identify the cohort who is likely to leave early enough so that the relationship can be saved.

### Data Set

This data is taken from a telecommunications company and involves customer data for a collection of customers who either stayed with the company or left within a certain period. <i>In many industries its often not the case that the cut off is so binary. Frequently it might be more likely that a client account lays dormant rather then getting explicitly closed - for example if the client only pays for usage. I will explain how to adjust for these situations latter in the piece.</i> This dataset is taken from [here] (http://www.dataminingconsultant.com/data/churn.txt) with descriptions of the data available [here] (http://www.sgi.com/tech/mlc/db/churn.names). This dataset also appears in the [C50 package] (http://cran.r-project.org/web/packages/C50/index.html)


Lets load the required packages:

```{r cache=TRUE, cache.path = 'Churn_cache/', fig.path='figure/', message=FALSE}
library(plyr)
library(dplyr)
library(stringr)
library(ggplot2)
library(reshape2)
library(caret)
```

Given that in this instance we have a binary current/former client example we can simply assess how many of each category 


Next up we need to split the data. The purpose of the split is to try and have a more accurate estimate of how our algorithm/s will go in production. If we were to assess the accuracy on the same data that we use to tune the algorithm, we will have an unrealistically optimistic accuracy. We can use the Caret package to split the data into a "Training Set" with 75% of the data and a "Test" Set with 25%.

```{r cache=TRUE, cache.path = 'Churn_cache/', fig.path='figure/'}
churn <- read.csv("D:/Users/soates/Downloads/churn.txt", header=T)
#churn <- read.csv("E:/Github Stuff/srepho.github.io/churn.txt", header=T)
set.seed(12)
trainIndex <- caret::createDataPartition(churn$Churn., p = .75, list = FALSE, times = 1)
churnTrain <- churn[ trainIndex,]
churnTest <- churn[-trainIndex, ]
```

So the first step is to have a look at the balance of the outcomes.

```{r cache=TRUE, cache.path = 'Churn_cache/', fig.path='figure/'}
table(churnTrain$Churn.)
```

So we can see from this that about ~15% of customers left our service. So we now have a baseline for naive prediction of ~85% if we just predict all customers will stay with us (which would not be very useful).

The next step is to have a look at the data - lets start by just looking at numerical summaries. The main things we are looking for are:

1. Missing Data: In particular is the missing data randomly spread or is affecting a particular cohort? Can we impute the data or should we just ignore it?

2. Errors: These are often of two kinds. Sometimes just obvious errors that stand out (someone might be listed as an impossible age like in our case less then say 16 and more then 85 might be suspicious) sometimes when the initial data was entered a "special" number is used to indicate its a missing field. Often this is something like 999999 or -99999 etc. and it really stands out from the other values.

3. Very low variance: Indicators that are nearly 100% one value are not likely to be very useful and in fact in many algorithms can have a detrimental affect. (If in doubt we can always run the algorithm twice and see which one does better).

4. The Shape of the Distribution: Some algorithms work more accurately if we can transform highly skewed data to more closely resemble a standard (Gaussian) distribution. Sometimes its as simple as a log transformation, sometimes we need something like a Box-Cox transformation. It is usually good practice to scale and centre our data as well as the difference in magnitude will trip up some algorithms.


```{r cache=TRUE, cache.path = 'Churn_cache/', fig.path='figure/'}
summary(churnTrain)
```

1. We can see that there are no missing data in this set (something that almost never happens in real life!). 

2. I cannot see any super obvious mistakes though some of the fields seem a little uncertain. In particular 

  * Its not clear what length of time these fields cover? I am guessing its an average of something (because otherwise the longer held accounts would have higher fields) but is it a daily, weekly or monthly average? Its hard to tell if these are unrealistic values without knowing the time covered. (As long as the data is correct though it won't matter what the ratio is as long as its standardised)
  * We need to have a closer look at the States and Phone Fields as we cannot tell if there are errors from this summary.
  
```{r cache=TRUE, cache.path = 'Churn_cache/', fig.path='figure/'}
table(churnTrain$State)
```

There is a big skew in terms of the number of Churned to non-Churned (only ~17% of people churn so even guessing all as non-Churned would return a success rate of about ~83%!). We can also see some features that are being treated as numerical that should not be (for example Area.Code is treated as a numerical factor when it should be a categorical factor as the numbers are not related to each other). The other factor that stands out is that Phone Field is not really usable as it currently stands as it seems that each phone number is singular. We will put this to one side for the moment. Latter we will look to see if we can modify it to extract something more meaningful.

So lets switch the Phone Field to a categorical factor.

```{r cache=TRUE, cache.path = 'Churn_cache/', fig.path='figure/'}
churnTrain$Area.Code<-as.factor(churnTrain$Area.Code)
churnTest$Area.Code<-as.factor(churnTest$Area.Code)
```

The next step is to have a close look at the variables graphically. I won't bore you with looking at every single one, though if the number of potential relationships is in the 100s this is something that I would do.


```{r fig.width=5, fig.height=5, cache=TRUE, cache.path = 'Churn_cache/', fig.path='figure/', fig.show='hold'}
one<-ggplot(churn, aes(x=Account.Length, fill=Churn.))+geom_density()+ facet_grid(Churn. ~ .) + labs(title="Account Length")
two<-ggplot(churn, aes(x=VMail.Message, fill=Churn.))+geom_density()+ facet_grid(Churn. ~ .) + labs(title="Voice Mails")
one 
two
```




<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-52218028-1', 'srepho.github.io');
  ga('send', 'pageview');

</script>

